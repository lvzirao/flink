1.DwsTradeCartAddUuWindow.java

“加购”是一个非常重要的行为指标，代表了用户对商品的购买意向。为了更好地分析用户行为趋势， 我们需要实时统计每天每秒新增的“加购独立用户数”，即：同一用户在一秒钟内多次加购只算一次。

输入数据：Kafka 中的 dwd 层加购日志（JSON 格式），包含 user_id 和 ts_ms 时间戳字段。 输出数据： 每秒窗口内新增的独立加购用户数量（去重）； 输出格式包括窗口起止时间、日期、加购独立用户数； 将结果写入 Doris 或控制台打印用于测试。

实现思路 数据清洗转换 从 Kafka 中读取 JSON 字符串，转为 JSONObject。 设置 Watermark 提取 ts_ms 作为事件时间，使用单调递增 Watermark。 按用户分组 使用 KeyedStream 对每个用户进行分组。 状态管理去重 利用 ValueState 记录用户上一次加购的日期； 如果当前加购日期不同于上次，则视为新用户，输出并更新状态； 设置状态 TTL 为一天，自动过期。 开窗聚合 使用 TumblingEventTimeWindow，每秒一个滚动窗口； 先聚合独立用户数（计数）； 再通过 AllWindowFunction 构建最终输出实体类 CartAddUuBean。 结果输出 转换为 JSON 字符串； 打印或写入 Doris。

2.DwsTradeProvinceOrderWindow.java

Kafka Source ↓ JSON 解析 + 过滤无效记录 ↓ KeyBy 订单ID去重（幂等处理） ↓ Watermark 设置事件时间 ↓ Map 转换为实体类对象 ↓ KeyBy 省份ID 分组 ↓ Tumbling Window 开窗聚合 ↓ Async Join 维度表 dim_base_province ↓ Sink To Doris

数据源读取与清洗 从 Kafka 读取订单明细日志； 忽略格式错误的数据，提高健壮性； 设置消费者组、初始偏移量等参数。
KeyBy + ProcessFunction 实现幂等处理 对 order_detail.id 做分组，保证同一订单只处理一次； 利用状态管理实现“幂等”机制，旧数据取反发送，新数据正常发送，防止重复累加； 设置 TTL 为 10 秒，避免状态无限增长。
设置 Watermark 并提取事件时间 使用 ts 字段作为事件时间； 使用单调递增的 Watermark，适用于大多数电商场景； 保障窗口计算的时间一致性。
Map 转换为实体类对象 构建统一的业务对象，方便下游处理； 使用 HashSet 存储订单 ID，用于去重计数。
KeyBy + TumblingWindow 开窗聚合 按省份分组； 使用 10 秒滚动窗口； 使用 ReduceFunction 累加金额，使用 WindowFunction 设置窗口时间维度。
异步关联维度表（Dim 表） 使用异步 IO 提高性能； 关联省份维度表获取省份名称； 支持并发访问外部维度服务（如 HBase / MySQL）。 为什么要对订单做幂等处理？ 因为 Kafka 消费可能重复消费，导致数据重复累加，需要幂等机制防止这种情况。 为什么使用 Async I/O 关联维度表？ 避免阻塞主线程，提高查询效率，适用于并发访问外部系统。 窗口函数使用 Reduce + WindowFunction 的组合有什么优势？ Reduce 函数轻量高效，适合中间聚合；WindowFunction 用于补充窗口元信息。 如果数据乱序严重怎么办？ 可以适当增加 Watermark 的延迟容忍时间，或者使用 Lateral Window。 如何保障 Exactly-Once？ 开启 Checkpointing，并且 Sink 支持事务提交。 3.DwsTradeSkuOrderWindow.java 1.主要用于对电商交易中的订单明细数据进行处理和分析。具体功能包括从 Kafka 中读取订单明细数据，对数据进行去重、时间窗口聚合， 然后关联多个维度表（如商品信息、品牌信息、分类信息等） 数据去重 按照订单明细的 id 进行分组，使用 Flink 的状态管理机制对重复数据进行去重处理。 时间窗口处理 指定水印和事件时间字段，按照商品 skuId 进行分组，然后使用滚动时间窗口（10 秒）对数据进行聚合。 维度关联 通过 AsyncDataStream.unorderedWait 方法异步关联多个维度表，包括商品信息表 dim_sku_info、商品品类表 dim_spu_info、品牌信息表 dim_base_trademark、三级分类表 dim_base_category3、二级分类表 dim_base_category2 和一级分类表 dim_base_category1。 数据输出 将最终处理结果转换为 JSON 字符串，并写入到 Doris 数据库的 dws_trade_sku_order_window 表中。 4.DwsTrafficHomeDetailPageViewWindow.java Kafka Source ↓ JSON 解析 + 过滤（仅保留 home 和 good_detail） ↓ 设置 Watermark + 提取事件时间 ↓ KeyBy mid 去重处理（判断是否当天首次访问） ↓ Map 转换为实体类对象 TrafficHomeDetailPageViewBean ↓ AllWindow 全局滚动窗口聚合（10秒） ↓ ReduceFunction + AllWindowFunction 聚合 UV ↓ Sink To Doris 本项目实现了以下能力：
实时性：基于 Flink 流式计算引擎，实现秒级 UV 统计； 精准性：通过 mid 分组 + 状态管理实现每日 UV 去重； 扩展性：模块化设计，易于接入其他页面类型； 稳定性：配置 Checkpoint、Restart、TTL 等机制，保障作业稳定运行。

安装oracle
1.下载oracle安装包
2.解压安装包
3.创建数据库用户
4.创建数据库
5.创建表空间
6.创建用户表空间
7.创建用户
8.创建用户权限
使用oracleSQL查询